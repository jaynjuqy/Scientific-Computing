{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309748ce-d284-476e-9150-c7207d91de84",
   "metadata": {},
   "source": [
    "# ICS 2207: SCIENTIFIC COMPUTING CAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a86897-c012-484f-902e-3234102d186b",
   "metadata": {},
   "source": [
    "## Question 1: Practical Gradient Descent Implementation (20 marks)\n",
    "You are provided with the function f(x) = x^2 - 6x + 9.\n",
    "\n",
    "a) Implement a Python function that performs gradient descent to minimize this function.\n",
    "The function should take as parameters:\n",
    "- the initial guess x0,\n",
    "- learning rate alpha,\n",
    "- and number of iterations n.\n",
    "The function should return the list of x-values and f(x)-values for each iteration. (10 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974ce17f-c6c4-4f8a-af72-d7412324590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "fn = x**2 - 6*x + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11dd0a4-691a-4e7b-b8d7-12eef91c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_prime = sp.diff(fn, x)\n",
    "\n",
    "def gradient_descent(x0: float, alpha: float, n: int):\n",
    "    x_values = [x0]\n",
    "    fx_values = [float(fn.subs(x, x0))]\n",
    "    x_value = x0\n",
    "    for it in range(n):\n",
    "        gradient = fn_prime.subs(x, x_value)\n",
    "        new_x_value = x_value - alpha * gradient\n",
    "        x_values.append(float(new_x_value))\n",
    "        fx_values.append(float(fn.subs(x, new_x_value)))\n",
    "        x_value = new_x_value\n",
    "\n",
    "    return x_values, fx_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a61fe-32b4-416a-b071-fabf9c9ff355",
   "metadata": {},
   "source": [
    "b) Run your function with x0 = 2, alpha = 0.1, and n = 10. Present the output as a table\n",
    "with columns: iteration, x-value, f(x)-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a81c300-242f-43ba-90ff-4ea7a47043e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: wcwidth in ./.env/lib/python3.13/site-packages (from prettytable) (0.2.13)\n",
      "Downloading prettytable-3.16.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b19150c-f485-48f1-b02a-29ea5aee3164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n",
      "| step | x value  | f(x) value |\n",
      "+------+----------+------------+\n",
      "|  0   |    2     |    1.0     |\n",
      "|  1   |   2.2    |    0.64    |\n",
      "|  2   |   2.36   |   0.4096   |\n",
      "|  3   |  2.488   |  0.262144  |\n",
      "|  4   |  2.5904  |  0.167772  |\n",
      "|  5   | 2.67232  |  0.107374  |\n",
      "|  6   | 2.737856 |  0.068719  |\n",
      "|  7   | 2.790285 |  0.04398   |\n",
      "|  8   | 2.832228 |  0.028147  |\n",
      "|  9   | 2.865782 |  0.018014  |\n",
      "|  10  | 2.892626 |  0.011529  |\n",
      "+------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "x_values, fx_values = gradient_descent(2, 0.1, 10)\n",
    "\n",
    "table = PrettyTable(['step', 'x value', 'f(x) value'])\n",
    "\n",
    "for i, (xv,fxv) in enumerate(zip(x_values, fx_values)):\n",
    "    table.add_row([i, round(xv, 6), round(fxv, 6)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ab2dd-1e00-4aa4-97e8-c9053f0a346b",
   "metadata": {},
   "source": [
    "c) Explain what would happen if the learning rate were set to 1 instead of 0.1, using insights from your code and outputs. (4mks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b2a326-295c-49ba-84b6-696aac735e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+\n",
      "| step | x value | f(x) value |\n",
      "+------+---------+------------+\n",
      "|  0   |    2    |    1.0     |\n",
      "|  1   |   4.0   |    1.0     |\n",
      "|  2   |   2.0   |    1.0     |\n",
      "|  3   |   4.0   |    1.0     |\n",
      "|  4   |   2.0   |    1.0     |\n",
      "|  5   |   4.0   |    1.0     |\n",
      "|  6   |   2.0   |    1.0     |\n",
      "|  7   |   4.0   |    1.0     |\n",
      "|  8   |   2.0   |    1.0     |\n",
      "|  9   |   4.0   |    1.0     |\n",
      "|  10  |   2.0   |    1.0     |\n",
      "+------+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "x_values, fx_values = gradient_descent(2, 1, 10)\n",
    "\n",
    "table = PrettyTable(['step', 'x value', 'f(x) value'])\n",
    "\n",
    "for i, (xv,fxv) in enumerate(zip(x_values, fx_values)):\n",
    "    table.add_row([i, round(xv, 6), round(fxv, 6)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd2580-385a-4f6f-8a21-ad7e58f5028b",
   "metadata": {},
   "source": [
    "As shown above, this would cause the x values to grow too fast, which would inevitably make us miss the actual x value that minimizes \n",
    "the function above. As seen the function values, denoted f(x) values in the table above, do not go down at all towards 0, instead, they\n",
    "remain at one because we are not converging toward the actual value that minimizes the function, rather just skipping over it. using a learning rate of 0.1, the x values were moving towards 3 while the fx values were converging towards o, which means that the actual value that minimizes the function was closer to 3. Using a learning rate of 1, we completely skip over this value, hence the function is not minimized at all. Implying that lower learning rates lead to better results, as it allows for smaller minor corrections to the x value to find the value that minimizes the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
